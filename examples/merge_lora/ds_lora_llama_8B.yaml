### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: /root/autodl-tmp/hub/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
adapter_name_or_path: /root/autodl-tmp/BigCreation/saves/DeepSeek-R1-7B-Distill/lora/train_r1_baseline/checkpoint-1300
template: deepseek3
trust_remote_code: true

### export
export_dir: /root/autodl-tmp/BigCreation/SFT_result/ds_lora_llama_8B
export_size: 5
export_device: cuda
export_legacy_format: false
