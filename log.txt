[2025-04-10 01:40:05,861] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-10 01:40:09,389] [INFO] [multinode_runner.py:85:get_cmd] Running on the following workers: asc250,asc251,asc252,asc253
[2025-04-10 01:40:09,389] [INFO] [runner.py:605:main] cmd = pdsh -S -f 1024 -w asc250,asc251,asc252,asc253 export PYTHONPATH=/home/zhangyan/LLaMA-Factory;  cd /home/zhangyan/LLaMA-Factory; /home/zhangyan/miniconda3/envs/llama-f/bin/python -u -m deepspeed.launcher.launch --world_info=eyJhc2MyNTAiOiBbMCwgMV0sICJhc2MyNTEiOiBbMCwgMV0sICJhc2MyNTIiOiBbMCwgMV0sICJhc2MyNTMiOiBbMCwgMV19 --node_rank=%n --master_addr=asc250 --master_port=12345 --enable_each_rank_log=None src/train.py --deepspeed config/ds_config.json --stage sft --do_train --model_name_or_path deepseek-ai/deepseek-llm-7b-base --finetuning_type lora --template default --dataset question_generation_squad --dataset_dir data --output_dir ./output --overwrite_cache --per_device_train_batch_size 2 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 5 --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 3 --bf16 True
asc253: [2025-04-10 01:40:12,629] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
asc253: Warning: The cache directory for DeepSpeed Triton autotune, /home/zhangyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
asc250: [2025-04-10 01:40:13,102] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
asc250: [2025-04-10 01:40:16,208] [INFO] [launch.py:146:main] WORLD INFO DICT: {'asc250': [0, 1], 'asc251': [0, 1], 'asc252': [0, 1], 'asc253': [0, 1]}
asc250: [2025-04-10 01:40:16,208] [INFO] [launch.py:152:main] nnodes=4, num_local_procs=2, node_rank=0
asc250: [2025-04-10 01:40:16,208] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'asc250': [0, 1], 'asc251': [2, 3], 'asc252': [4, 5], 'asc253': [6, 7]})
asc250: [2025-04-10 01:40:16,208] [INFO] [launch.py:164:main] dist_world_size=8
asc250: [2025-04-10 01:40:16,208] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
asc250: [2025-04-10 01:40:16,209] [INFO] [launch.py:256:main] process 3238788 spawned with command: ['/home/zhangyan/miniconda3/envs/llama-f/bin/python', '-u', 'src/train.py', '--local_rank=0', '--deepspeed', 'config/ds_config.json', '--stage', 'sft', '--do_train', '--model_name_or_path', 'deepseek-ai/deepseek-llm-7b-base', '--finetuning_type', 'lora', '--template', 'default', '--dataset', 'question_generation_squad', '--dataset_dir', 'data', '--output_dir', './output', '--overwrite_cache', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--logging_steps', '5', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3', '--bf16', 'True']
asc250: [2025-04-10 01:40:16,209] [INFO] [launch.py:256:main] process 3238789 spawned with command: ['/home/zhangyan/miniconda3/envs/llama-f/bin/python', '-u', 'src/train.py', '--local_rank=1', '--deepspeed', 'config/ds_config.json', '--stage', 'sft', '--do_train', '--model_name_or_path', 'deepseek-ai/deepseek-llm-7b-base', '--finetuning_type', 'lora', '--template', 'default', '--dataset', 'question_generation_squad', '--dataset_dir', 'data', '--output_dir', './output', '--overwrite_cache', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--logging_steps', '5', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3', '--bf16', 'True']
asc253: [2025-04-10 01:40:16,253] [INFO] [launch.py:146:main] WORLD INFO DICT: {'asc250': [0, 1], 'asc251': [0, 1], 'asc252': [0, 1], 'asc253': [0, 1]}
asc253: [2025-04-10 01:40:16,253] [INFO] [launch.py:152:main] nnodes=4, num_local_procs=2, node_rank=3
asc253: [2025-04-10 01:40:16,253] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'asc250': [0, 1], 'asc251': [2, 3], 'asc252': [4, 5], 'asc253': [6, 7]})
asc253: [2025-04-10 01:40:16,253] [INFO] [launch.py:164:main] dist_world_size=8
asc253: [2025-04-10 01:40:16,253] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
asc253: [2025-04-10 01:40:16,254] [INFO] [launch.py:256:main] process 1026759 spawned with command: ['/home/zhangyan/miniconda3/envs/llama-f/bin/python', '-u', 'src/train.py', '--local_rank=0', '--deepspeed', 'config/ds_config.json', '--stage', 'sft', '--do_train', '--model_name_or_path', 'deepseek-ai/deepseek-llm-7b-base', '--finetuning_type', 'lora', '--template', 'default', '--dataset', 'question_generation_squad', '--dataset_dir', 'data', '--output_dir', './output', '--overwrite_cache', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--logging_steps', '5', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3', '--bf16', 'True']
asc253: [2025-04-10 01:40:16,254] [INFO] [launch.py:256:main] process 1026760 spawned with command: ['/home/zhangyan/miniconda3/envs/llama-f/bin/python', '-u', 'src/train.py', '--local_rank=1', '--deepspeed', 'config/ds_config.json', '--stage', 'sft', '--do_train', '--model_name_or_path', 'deepseek-ai/deepseek-llm-7b-base', '--finetuning_type', 'lora', '--template', 'default', '--dataset', 'question_generation_squad', '--dataset_dir', 'data', '--output_dir', './output', '--overwrite_cache', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--logging_steps', '5', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3', '--bf16', 'True']
asc251: [2025-04-10 01:40:17,582] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
asc252: [2025-04-10 01:40:17,925] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
asc251: Warning: The cache directory for DeepSpeed Triton autotune, /home/zhangyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
asc252: Warning: The cache directory for DeepSpeed Triton autotune, /home/zhangyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
asc253: [2025-04-10 01:40:20,728] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
asc253: [2025-04-10 01:40:20,730] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
asc253: Warning: The cache directory for DeepSpeed Triton autotune, /home/zhangyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
asc253: Warning: The cache directory for DeepSpeed Triton autotune, /home/zhangyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
asc250: [2025-04-10 01:40:21,272] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
asc250: [2025-04-10 01:40:21,273] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
asc253: [2025-04-10 01:40:22,559] [INFO] [comm.py:658:init_distributed] cdb=None
asc253: [2025-04-10 01:40:22,561] [INFO] [comm.py:658:init_distributed] cdb=None
asc250: [2025-04-10 01:40:23,260] [INFO] [comm.py:658:init_distributed] cdb=None
asc250: [2025-04-10 01:40:23,264] [INFO] [comm.py:658:init_distributed] cdb=None
asc250: [2025-04-10 01:40:23,264] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
asc253: [WARNING|2025-04-10 01:40:23] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
asc253: [INFO|2025-04-10 01:40:23] llamafactory.hparams.parser:379 >> Process rank: 6, world size: 8, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
asc253: [INFO|2025-04-10 01:40:24] llamafactory.hparams.parser:379 >> Process rank: 7, world size: 8, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
asc250: [INFO|2025-04-10 01:40:27] llamafactory.hparams.parser:379 >> Process rank: 1, world size: 8, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
